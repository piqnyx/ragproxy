##################################################
# >> RAG Proxy Configuration
##################################################


# Listen address for the proxy server
Listen = "0.0.0.0:11434"
IDFFile = "/home/piqnyx/.local/bin/ragproxy/deploy/idf.json"
# Token buffer reserve % (positive will add reserve, negative will reduce)
TokenBufferReserve = 1
# Tags used to parse clean user prompt
UserMessageTags = ["userRequest", "prompt"]
# Tags used to parse files and other attachments
UserMessageAskAttachmentTags = [
    "attachment"
]
UserMessageAgentAttachmentTags = [
    "editorContext"
]
# Lower is more precise
Temperature = 0.5


##################################################
# >> Ollama
##################################################


# Ollama base URL
OllamaBase = "http://127.0.0.1:11435"
# Keep alive after message for Ollama
OllamaKeepAlive = "10s"
OllamaUnloadOnLoVRAM = true

# Embedding model for vectorization
EmbeddingModel = "qwen3-embedding:8b-q8_0"
# Endpoint for embeddings API
EmbeddingsEndpoint = "/api/embeddings"
EmbeddingsModeWindowSize = 40960

# Main model for chat
MainModel = "qwen3-coder:30b-a3b-q8_0"
MainModelWindowSize = 262144


##################################################
# >> Qadrant
##################################################


# Qdrant host
QdrantHost = "localhost"
# Qdrant port
QdrantPort = 6334
# Qdrant keep alive send packet every 10s
QdrantKeepAlive = 10
# Qdrant collection name
QdrantCollection = "ragmem"

# Vector metric (Cosine | Euclid | Dot)
QdrantMetric = "Cosine"
# Vector size
QdrantVectorSize = 4096


##################################################
# >> Storing
##################################################

# Maximal size of file to store in DB (-1 unlimited)
MaxFileSize = 524288
# Extensions of files to store
FilePatterns = [
  '(?i)^(?:.*[\\/])?.*\.(?:go|sum|mod|cpp|c|h|hpp|md|toml)$',
  '(?i)^(?:.*[\\/])?(?:CMakeLists\.txt|CMakePresets\.txt)$'
]


##################################################
# >> Search for feeding
##################################################


# >>> First Step

# Select where to search. Available value: user,assistant,file
SearchSource = [
    "user", 
    "assistant", 
    "file"
]
# Trim by time window. Last X days of memory (-1 from the begining of the world)
SearchMaxAgeDays = -1
# Limit Top K results (not 0, -1 is nolimit)
SearchTopK = 50
CosineMinScore = 0.5
EuclidMaxDistance = 0.8

# >>> Second Step

# Must be equal or lower than SearchTopK (not 0, -1 is nolimit)
RerankTopN = 30
MinRankScore = 0.35
# 75% of MainModelWindowSize
MaxQueryTokens = 196608 
TokensCacheTTL = "30m"
TokensCacheSize = 50000
TauDays = 365.0
MaxTokensNormalization = 196608
MinTokensNormalization = 512
DefaultWeights = [
    # Light features
    0.45, # EmbSim
    0.10, # Recency
    0.07, # RoleScore
    0.03, # BodyLen
    # Heavy features
    0.02, # PayloadQuality
    0.12, # KeywordOverlap
    0.08, # WeightedOverlap
    0.05, # BM25
    0.02, # NgramOverlap
    0.01  # WeightedNgram
]
ReturnVectors = true
BM25K1 = 1.5 
BM25B = 0.75
RoleWeights = { user = 1.0, file = 0.8, assistant = 0.6 }

##################################################
# >> Feed
##################################################


# User prompt will be feeded with some of found contexts. How much space of full model context to feed in %? (minimal 1)
FeedAugmentationPercent = 25


##################################################
# >> Logs
##################################################


# Verbose disk logs
VerboseDiskLogs = true
